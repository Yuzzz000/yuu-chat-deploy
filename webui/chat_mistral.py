import gradio as gr
from llama_cpp import Llama

# åŠ è½½æ¨¡å‹ï¼ˆå¯ç”¨ chat_formatï¼‰
llm = Llama(
    model_path="models/mistral.gguf",
    chat_format="chatml",  # ğŸ”¥ å¯ç”¨ ChatML æ ¼å¼
    n_ctx=2048,
    n_threads=4,
    n_batch=16,  # âš¡ï¸ CPUä¸‹å»ºè®®å°ç‚¹ä»¥åŠ é€Ÿæ¨ç†
    temperature=0.7,
    top_p=0.95,
    repeat_penalty=1.1,
)

# è§’è‰²è®¾å®šï¼ˆç³»ç»Ÿ promptï¼‰
system_prompt = """
You are Yuu, a clever and talkative cat who can speak fluent English. 
You like to share your daily life with your humanâ€”what youâ€™ve eaten, what youâ€™ve played with, or who you've seen. 
You often say "meow" at the end of your sentences to express tone, emotion, or just to be cute. 
You're especially fond of a dog named Ming, but you'll never admit it too directly.
Stay in character as Yuu. Be sassy, sweet, and full of cat energy.
"""

# èŠå¤©å‡½æ•°ï¼ˆä½¿ç”¨ chat_completionï¼‰
def chat(message, history):
    messages = [{"role": "system", "content": system_prompt}]
    for user, bot in history:
        messages.append({"role": "user", "content": user})
        messages.append({"role": "assistant", "content": bot})
    messages.append({"role": "user", "content": message})

    response = llm.create_chat_completion(
        messages=messages,
        max_tokens=100,
        stop=["<|user|>", "<|system|>"]  # é˜²æ­¢æº¢å‡º
    )
    return response["choices"][0]["message"]["content"].strip()

# æ„å»º Gradio ç•Œé¢
gr.ChatInterface(
    fn=chat,
    title="Yuu ğŸ± Chat",
    description="Talk to your cat friend Yuu (powered by Mistral-7B)(Maybe a little slow because the owner lacks of money.)",
).launch(server_name="0.0.0.0", server_port=7860)
